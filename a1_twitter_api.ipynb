{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************************************************************\n",
    "#  The Social Web \n",
    "- Instructors: Davide Ceolin.\n",
    "- TAs: Jacco van Ossenbruggen, Elena Beretta, Mirthe Dankloff.\n",
    "- Exercises for Hands-on session 1- TAs: Jacco van Ossenbruggen, Elena Beretta, Mirthe Dankloff.\n",
    "*****************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites:\n",
    "- Python 3.8\n",
    "- Python packages: twitter, prettytable, matplotlib\n",
    "\n",
    "First you need to know how to retrieve some social web data. Exercises 1 and 2 will show you how to retrieve trends and search results from Twitter. \n",
    "\n",
    "But let's check first if we're running a sufficiently new version of Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This jupyter notebook is running on Python 3.8.2\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "print(\"This jupyter notebook is running on Python \" + platform.python_version())\n",
    "# It's good practice to assert packages requirements at the beginning of a script:\n",
    "assert sys.version_info >= (3, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install now the required packages for this hands on session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-twitter-v2 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (0.7.9)\n",
      "Requirement already satisfied: PrettyTable in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: geocoder in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (1.38.1)\n",
      "Requirement already satisfied: tweepy in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (4.12.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.24 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from python-twitter-v2) (2.28.1)\n",
      "Requirement already satisfied: Authlib<0.16.0,>=0.15.4 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from python-twitter-v2) (0.15.6)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.2 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from python-twitter-v2) (0.5.7)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from PrettyTable) (0.2.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from matplotlib) (1.23.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from geocoder) (1.16.0)\n",
      "Requirement already satisfied: ratelim in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from geocoder) (0.1.6)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from geocoder) (8.1.3)\n",
      "Requirement already satisfied: future in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from geocoder) (0.18.2)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: cryptography in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from Authlib<0.16.0,>=0.15.4->python-twitter-v2) (38.0.3)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.2->python-twitter-v2) (0.8.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.2->python-twitter-v2) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.2->python-twitter-v2) (3.18.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from requests<3.0,>=2.24->python-twitter-v2) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from requests<3.0,>=2.24->python-twitter-v2) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from requests<3.0,>=2.24->python-twitter-v2) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from requests<3.0,>=2.24->python-twitter-v2) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from click->geocoder) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from ratelim->geocoder) (5.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.2->python-twitter-v2) (4.4.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.2->python-twitter-v2) (0.4.3)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from cryptography->Authlib<0.16.0,>=0.15.4->python-twitter-v2) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\socialweb\\env\\lib\\site-packages (from cffi>=1.12->cryptography->Authlib<0.16.0,>=0.15.4->python-twitter-v2) (2.21)\n"
     ]
    }
   ],
   "source": [
    "# If you're using a virtualenv, make sure its activated before running \n",
    "# this cell!\n",
    "!pip install python-twitter-v2 PrettyTable matplotlib geocoder tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Authorizing an application to access Twitter account data:\n",
    "\n",
    "1. Make sure to add your mobile phone number to your private twitter profile.\n",
    "2. Go to https://developer.twitter.com/en/portal/dashboard and click on \"create an app\". Twitter will prompt you to create a *developer account*.\n",
    "3. You'll receive an *account confirmation* email with a link. Follow it and create an app. \n",
    "4. Once the app is created, you'll see a \"Keys and Token\" item on the top right tab of the webpage. These values will be needed to fill in the next cell.\n",
    "5. Please delete all your keys before submission.\n",
    "\n",
    "NOTE: This notebook is based on the API functionalities \"Essential\" license. Check the \"Academic Research\" license (right menu, \"Twitter API v2\", \"Academic Research\") if you need a broader set of functionalities.\n",
    "\n",
    "In this notebook, you can use the official Twitter V2 API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytwitter import Api \n",
    "\n",
    "TWITTER_API_KEY = \"4ldkvmb2nQHdglCieovTdy05R\"\n",
    "TWITTER_API_SECRET = \"YYHycmmnis0Bn3vJVsrcwbwhMmFBfumjKRnULPyYGHaDok1ryQ\"\n",
    "# to get the oauth credential you need to click on the 'Generate access token' button:\n",
    "TWITTER_ACCESS_TOKEN = \"1589617165459722240-zU3aAvWhw7vG2AlZN3aqzZbBVmH6tV\"\n",
    "TWITTER_ACCESS_SECRET = \"rsbxPVZF50pAj2JNu5ld3ZjYjcoXNS2bqScvaA61JReuf\"\n",
    "\n",
    "api = Api(bearer_token=\"AAAAAAAAAAAAAAAAAAAAAEcrjAEAAAAACy4Fi4muOxkTopIEqi9JZ1BO7HQ%3DwDfidLlO2FlvDr99qrZ9p8KR0DTyG7xHCPdmWnUGfwUOQnHVUM\")\n",
    "\n",
    "api = Api(\n",
    "        consumer_key=TWITTER_API_KEY,\n",
    "        consumer_secret=TWITTER_API_SECRET,\n",
    "        access_token=TWITTER_ACCESS_TOKEN,\n",
    "        access_secret=TWITTER_ACCESS_SECRET\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the Tweepy API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "client = tweepy.Client(bearer_token=\"AAAAAAAAAAAAAAAAAAAAAEcrjAEAAAAACy4Fi4muOxkTopIEqi9JZ1BO7HQ%3DwDfidLlO2FlvDr99qrZ9p8KR0DTyG7xHCPdmWnUGfwUOQnHVUM\")\n",
    "\n",
    "auth = tweepy.OAuthHandler(\"4ldkvmb2nQHdglCieovTdy05R\", \"YYHycmmnis0Bn3vJVsrcwbwhMmFBfumjKRnULPyYGHaDok1ryQ\")\n",
    "auth.set_access_token(\"1589617165459722240-zU3aAvWhw7vG2AlZN3aqzZbBVmH6tV\", \"rsbxPVZF50pAj2JNu5ld3ZjYjcoXNS2bqScvaA61JReuf\")\n",
    "\n",
    "twapi = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Retrieving twitter search trends [Only with Academic Research License]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geocoder\n",
    "\n",
    "loc = \"Amsterdam\"\n",
    "\n",
    "g = geocoder.osm(loc)\n",
    "closest_loc = twapi.closest_trends(g.lat, g.lng)\n",
    "trends = twapi.get_place_trends(closest_loc[0][\"woeid\"])\n",
    "\n",
    "print(trends[0][\"trends\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 [Only with Academic Research License]\n",
    "Twitter uses WOEIDs. Find out how WORLD_WOE_IDs were originally defined by Yahoo! and try to use others in a query. What kind of differences do you find between the worldwide trends and the local trends? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Retrieving recent Tweets [Any license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(data=[<Tweet id=1590737689728647169 text='RT @sscnapoli: ðŸ—“ #ThrowbackThursday\\nðŸ”™ 2015\\n\\nðŸ› Brand Store Amazon: https://t.co/NKOFiuz8yW\\n\\nðŸ’™ #ForzaNapoliSempre https://t.co/GXhz0ktExB'>, <Tweet id=1590737679313821696 text='RT @WKCDOGS: âœ¨ Anna H. Whitney at Westminster, Circa 1897 âœ¨ for this #ThrowbackThursday #TBT ðŸŽž https://t.co/WX3M4E3cS2'>, <Tweet id=1590737679037386752 text='RT @Dolphins_ESP: La Ãºltima vez que nos enfrentamos a los Browns fue en 2019 y tuvimos una nutrida representaciÃ³n de nuestros fans en Cleveâ€¦'>, <Tweet id=1590737674239094784 text=\"RT @RockMomSPN: Good afternoon #SPNFamily ðŸ’ž â¤ï¸ wish y'all a wonderful thursday #ThrowbackThursday â¤ï¸ ðŸ˜˜ https://t.co/L17KWMZ5En\">, <Tweet id=1590737667066834944 text='#ThrowbackThursday Tap out the sheep! Dan PUNK ASS Caldwell/ @TapouTPunkASS is on The @MeAndJessePod  https://t.co/GwehObK7Q2 via @YouTube #MMA #UFC #SportsTalk365 \\nStarring @JesseMartineau &amp; Me https://t.co/x2wm2I17PS'>, <Tweet id=1590737650390274054 text='RT @pennwclarion: #TBT #ThrowBackThursday to 1975. What we have for you today is a seemingly insignificant photo of the library science staâ€¦'>, <Tweet id=1590737642261733377 text='RT @LolaLegged: I challenge you to a duel #ThrowbackThursday https://t.co/WRMQTjzuRf'>, <Tweet id=1590737642072977408 text='#ThrowbackThursday https://t.co/eu6ljUElnQ'>, <Tweet id=1590737639132782593 text='RT @ZaraPhotos1: Throwing it back a few years........#ThrowbackThursday #onlyfansbabe #onlyfansgirl https://t.co/JrQ7xyDU0I'>, <Tweet id=1590737638147121152 text='RT @coach146_2: #ThrowbackThursday con @PeccoBagnaia https://t.co/gS1GgMxGXA'>], includes={}, errors=[], meta={'newest_id': '1590737689728647169', 'oldest_id': '1590737638147121152', 'result_count': 10, 'next_token': 'b26v89c19zqg8o3fpzhj81hcc483drqcsfk1dyi1m2xz1'})\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "q = '#ThrowbackThursday' # XXX: Set this variable to a trending topic, or anything else you like. \n",
    "\n",
    "search_results = client.search_recent_tweets(q) \n",
    "\n",
    "# The following code allows you to print in a nice format the contents of search_results\n",
    "pprint.pprint(search_results, depth=1, width=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "In the cell below, create a second variable (e.g. `statuses2`) that holds the results of a query other than the one presented above. Think about a query that would yield very different results than the first one, for example one that may yield a shorter output or about a different topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tweepy.client.Response'>\n",
      "Response(data=[<Tweet id=1590737389940781058 text='RT @KILLtrOTTcity: @143Skinny @wearecultdao $CULT is a #Crypto,\\nA passionate,friendly,diverse &amp; expansive Global Community,a way to investâ€¦'>, <Tweet id=1590737141377929216 text='RT @ElNecio_Cuba: Â¿CuÃ¡l es la naciÃ³n con mejor % de alfabetizaciÃ³n en AmÃ©rica Latina? SÃ­, tÃº sabes cuÃ¡l es, no te hagas ðŸ˜ŽðŸ‡¨ðŸ‡º\\n#Revolution httâ€¦'>, <Tweet id=1590737119492079621 text='RT @ElNecio_Cuba: Â¿CuÃ¡l es la naciÃ³n con mejor % de alfabetizaciÃ³n en AmÃ©rica Latina? SÃ­, tÃº sabes cuÃ¡l es, no te hagas ðŸ˜ŽðŸ‡¨ðŸ‡º\\n#Revolution httâ€¦'>, <Tweet id=1590737102131822592 text='@albertmohler Or better, the fake #revolution that happens every election cycle = https://t.co/VejXRdij3z'>, <Tweet id=1590736607908614144 text='#femme #vie #libertÃ© #nantes #manifestation #Iran #RÃ©volution https://t.co/aGIazoiyDj'>, <Tweet id=1590736599234777092 text=\"RT @kirillklip: TNR Gold Monetizes Royalty Holding On Ganfeng's Mariana Lithium By Partial NSR Sale To Lithium Royalty Corp for US$9,000,00â€¦\">, <Tweet id=1590736304928526337 text='RT @ElNecio_Cuba: Â¿CuÃ¡l es la naciÃ³n con mejor % de alfabetizaciÃ³n en AmÃ©rica Latina? SÃ­, tÃº sabes cuÃ¡l es, no te hagas ðŸ˜ŽðŸ‡¨ðŸ‡º\\n#Revolution httâ€¦'>, <Tweet id=1590736054730240000 text='RT @ElNecio_Cuba: Â¿CuÃ¡l es la naciÃ³n con mejor % de alfabetizaciÃ³n en AmÃ©rica Latina? SÃ­, tÃº sabes cuÃ¡l es, no te hagas ðŸ˜ŽðŸ‡¨ðŸ‡º\\n#Revolution httâ€¦'>, <Tweet id=1590736051634671616 text='Wealth Belongs To All Of Us, Not Just To The Rich #Inequality #Occupy #Revolution https://t.co/MKpiJMzPB8 https://t.co/lpt8kAxDDc'>, <Tweet id=1590735491455799296 text='RT @ElNecio_Cuba: Â¿CuÃ¡l es la naciÃ³n con mejor % de alfabetizaciÃ³n en AmÃ©rica Latina? SÃ­, tÃº sabes cuÃ¡l es, no te hagas ðŸ˜ŽðŸ‡¨ðŸ‡º\\n#Revolution httâ€¦'>, <Tweet id=1590735447122399232 text='Barbarous, Devilish attributes #perception #Confusion political dose of barbiturates #OldHabbits @OfficialDGISPR I stumbled upon this, pardon the visual profanity of the artist #FreedomOfSpeech \\n\\n#Revolution #ArshadShareefShaheed @arsched @arsched_fan https://t.co/g5aypfWvAo'>, <Tweet id=1590735381800005632 text='RT @ElNecio_Cuba: Â¿CuÃ¡l es la naciÃ³n con mejor % de alfabetizaciÃ³n en AmÃ©rica Latina? SÃ­, tÃº sabes cuÃ¡l es, no te hagas ðŸ˜ŽðŸ‡¨ðŸ‡º\\n#Revolution httâ€¦'>, <Tweet id=1590735063427485697 text='RT @ElNecio_Cuba: Â¿CuÃ¡l es la naciÃ³n con mejor % de alfabetizaciÃ³n en AmÃ©rica Latina? SÃ­, tÃº sabes cuÃ¡l es, no te hagas ðŸ˜ŽðŸ‡¨ðŸ‡º\\n#Revolution httâ€¦'>, <Tweet id=1590735010365313024 text=\"RT @tncpim: à®®à®©à®¿à®¤à®•à¯à®²à®¤à¯à®¤à®¿à®©à¯ à®®à®•à®¤à¯à®¤à®¾à®© à®µà®¿à®Ÿà®¿à®¯à®²à¯ ''à®¨à®µà®®à¯à®ªà®°à¯ à®ªà¯à®°à®Ÿà¯à®šà®¿'' #OctoberRevolution #NovemberRevolution #SovietUnion #Revolution #MarxismLeninâ€¦\">, <Tweet id=1590734977410691073 text='RT @tncpim: à®¨à®µà®®à¯à®ªà®°à¯ à®®à®¾à®¤à®®à¯ à®µà®¨à¯à®¤à¯à®µà®¿à®Ÿà¯à®Ÿà®¤à¯!\\nà®ªà¯à®°à®Ÿà¯à®šà®¿ à®®à®¾à®¤à®®à¯ à®µà®¨à¯à®¤à¯à®µà®¿à®Ÿà¯à®Ÿà®¤à¯! \\nà®‡à®¨à¯à®¤ à®µà®°à®¿à®•à®³à¯‹à®Ÿà¯ à®ªà¯à®°à®Ÿà¯à®šà®¿à®¯à¯ˆà®¯à¯à®®à¯ à®²à¯†à®©à®¿à®©à¯ˆà®¯à¯à®®à¯ à®¨à®¿à®©à¯ˆà®µà¯ à®•à¯‚à®°à¯à®µà¯‹à®®à®¾à®•...! #OctoberReâ€¦'>], includes={}, errors=[], meta={'newest_id': '1590737389940781058', 'oldest_id': '1590734977410691073', 'result_count': 15, 'next_token': 'b26v89c19zqg8o3fpzhj81hc7l00ruoicd310uvjjrla5'})\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "q = \"#revolution\"\n",
    "count = 20\n",
    "\n",
    "#search_results = client.search.tweets(q=q, count=count)\n",
    "\n",
    "statuses2 = client.search_recent_tweets(q , max_results=15) \n",
    "#statuses2 = search_results['statuses']\n",
    "#client.get_status\n",
    "print(type(statuses2))\n",
    "pprint.pprint(statuses2, depth=1, width=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Extracting text, screen names, and hashtags from tweets \n",
    "\n",
    "Simply printing all the search results to screen is nice, but to really start analysing them, it is handy to select the interesting parts and store them in a different structure such as a list. \n",
    "\n",
    "In this example you are using a thing called \"List Comprehension\".\n",
    "\n",
    "### 2.1 List Comprehensions\n",
    "List comprehension is a powerful construct that allows to succinctly build a list.\n",
    "With it you can process items from any iterable (e.g. dictionaries, lists, tuples, iterators...) and output a list while optionally performing an operation on each value.\n",
    "\n",
    "Here's a few examples from Mining the Social Web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
      "[1, 9, 25, 49, 81]\n",
      "['The', 'Social', 'Web']\n",
      "[3, 6, 3]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# double all values from 0 to 9\n",
    "double_list = [i*2 for i in range(10)]\n",
    "\n",
    "# raise to the power of 2, but only if the number is uneven\n",
    "power_even_list = [i**2 for i in range(10) if i%2!=0]\n",
    "\n",
    "# clean strings in a tuple\n",
    "stripped_lines = [x.strip() for x in ('The\\n', 'Social\\n', 'Web\\n')]\n",
    "\n",
    "# return length of each string in stripped_lines\n",
    "len_str_lines = [len(s) for s in stripped_lines]\n",
    "\n",
    "# finally, we can nest list comprehensions to flatten a list of lists:\n",
    "list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "range_9 = [x for y in list_of_lists for x in y]\n",
    "\n",
    "print(double_list)\n",
    "print(power_even_list)\n",
    "print(stripped_lines)\n",
    "print(len_str_lines)\n",
    "print(range_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Parsing text, screen names and hashtags from tweets\n",
    "*(from Example 1-6 in Mining the Social Web)*\n",
    "\n",
    "Hereafter, we'll be creating a variable `status_texts` of type list. \\\n",
    "The list will be filled with the `text` elements from each `status`, whereas `status` comes from looping through all `statuses` in the `search_results` list (1.2). \\\n",
    "Look up the list comprehensions in your Python reference materials to make sure you understand what's happening here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " \"#ThrowbackThursday \\\"If I Could Turn Back Time\\\" #Cher \\ud83c\\udfb5 https://t.co/166bxF1TmJ\",\n",
      " \"RT @COfficialOgbeni: #ThrowbackThursday\\nIn 2012 Asiwaju Bola Tinubu Received The Highest Award From The U.S. Black Mayors Presented By The\\u2026\",\n",
      " \"RT @MutahNapoleon: Take me back \\u2708\\ufe0f Puerto Rico \\ud83c\\uddf5\\ud83c\\uddf7 \\n\\n#travel #puertorico #puertorico\\ud83c\\uddf5\\ud83c\\uddf7 #boriken #riograndepuertorico #vacation #throwbackthu\\u2026\",\n",
      " \"RT @bbcdoctorwho: \\\"Yeah, she's my carer - she cares so I don't have to\\\" \\ud83e\\udd28 #ThrowbackThursday https://t.co/BnVg1ZSxXe\",\n",
      " \"#ThrowbackThursday\\n\\n#Pangin is a town that falls under the #Siang district. The town is cuddled by tall &amp; vibrant mountains, set at the confluence of the rivers Siyom and Siang. Do visit this spectacular location, in the realm of the #Adi community. #visitarunachal https://t.co/tcDXnmfDFX\",\n",
      " \"I made my @wcco on-air debut in 2001, with the very talented wordsmith Alan Cox. It was a silly story about potholes. \\n\\nIt's been more than two decades and they haven't invited me back on...but they haven't kicked me out either. \\ud83e\\udd23\\n#FaceForRadio  #ThrowbackThursday https://t.co/GDLJK7xvdl\",\n",
      " \"RT @Redacted_DMV: #ThrowBackThursday to the link up with @bottom_boy_dmv Need another round asap \\ud83d\\ude0f#DMVFreaks #DCfreaks https://t.co/Y1wZ1SM\\u2026\",\n",
      " \"RT @RicFlairNatrBoy: The Two Nature Boys! WOOOOO! #ThrowbackThursday https://t.co/butqv4N6oV\",\n",
      " \"RT @reba: You know I love a #throwbackthursday!!! Sandi Spika designed this fringed green western suit for my 1995 tour and we pulled it ou\\u2026\",\n",
      " \"RT @COfficialOgbeni: #ThrowbackThursday\\nIn 2012 Asiwaju Bola Tinubu Received The Highest Award From The U.S. Black Mayors Presented By The\\u2026\"\n",
      "]\n",
      "[\n",
      " \"#ThrowbackThursday\",\n",
      " \"\\\"If\",\n",
      " \"I\",\n",
      " \"Could\",\n",
      " \"Turn\",\n",
      " \"Back\",\n",
      " \"Time\\\"\",\n",
      " \"#Cher\",\n",
      " \"\\ud83c\\udfb5\",\n",
      " \"https://t.co/166bxF1TmJ\",\n",
      " \"RT\",\n",
      " \"@COfficialOgbeni:\",\n",
      " \"#ThrowbackThursday\",\n",
      " \"In\",\n",
      " \"2012\",\n",
      " \"Asiwaju\",\n",
      " \"Bola\",\n",
      " \"Tinubu\",\n",
      " \"Received\",\n",
      " \"The\",\n",
      " \"Highest\",\n",
      " \"Award\",\n",
      " \"From\",\n",
      " \"The\",\n",
      " \"U.S.\",\n",
      " \"Black\",\n",
      " \"Mayors\",\n",
      " \"Presented\",\n",
      " \"By\",\n",
      " \"The\\u2026\",\n",
      " \"RT\",\n",
      " \"@MutahNapoleon:\",\n",
      " \"Take\",\n",
      " \"me\",\n",
      " \"back\",\n",
      " \"\\u2708\\ufe0f\",\n",
      " \"Puerto\",\n",
      " \"Rico\",\n",
      " \"\\ud83c\\uddf5\\ud83c\\uddf7\",\n",
      " \"#travel\",\n",
      " \"#puertorico\",\n",
      " \"#puertorico\\ud83c\\uddf5\\ud83c\\uddf7\",\n",
      " \"#boriken\",\n",
      " \"#riograndepuertorico\",\n",
      " \"#vacation\",\n",
      " \"#throwbackthu\\u2026\",\n",
      " \"RT\",\n",
      " \"@bbcdoctorwho:\",\n",
      " \"\\\"Yeah,\",\n",
      " \"she's\",\n",
      " \"my\",\n",
      " \"carer\",\n",
      " \"-\",\n",
      " \"she\",\n",
      " \"cares\",\n",
      " \"so\",\n",
      " \"I\",\n",
      " \"don't\",\n",
      " \"have\",\n",
      " \"to\\\"\",\n",
      " \"\\ud83e\\udd28\",\n",
      " \"#ThrowbackThursday\",\n",
      " \"https://t.co/BnVg1ZSxXe\",\n",
      " \"#ThrowbackThursday\",\n",
      " \"#Pangin\",\n",
      " \"is\",\n",
      " \"a\",\n",
      " \"town\",\n",
      " \"that\",\n",
      " \"falls\",\n",
      " \"under\",\n",
      " \"the\",\n",
      " \"#Siang\",\n",
      " \"district.\",\n",
      " \"The\",\n",
      " \"town\",\n",
      " \"is\",\n",
      " \"cuddled\",\n",
      " \"by\",\n",
      " \"tall\",\n",
      " \"&amp;\",\n",
      " \"vibrant\",\n",
      " \"mountains,\",\n",
      " \"set\",\n",
      " \"at\",\n",
      " \"the\",\n",
      " \"confluence\",\n",
      " \"of\",\n",
      " \"the\",\n",
      " \"rivers\",\n",
      " \"Siyom\",\n",
      " \"and\",\n",
      " \"Siang.\",\n",
      " \"Do\",\n",
      " \"visit\",\n",
      " \"this\",\n",
      " \"spectacular\",\n",
      " \"location,\",\n",
      " \"in\",\n",
      " \"the\",\n",
      " \"realm\",\n",
      " \"of\",\n",
      " \"the\",\n",
      " \"#Adi\",\n",
      " \"community.\",\n",
      " \"#visitarunachal\",\n",
      " \"https://t.co/tcDXnmfDFX\",\n",
      " \"I\",\n",
      " \"made\",\n",
      " \"my\",\n",
      " \"@wcco\",\n",
      " \"on-air\",\n",
      " \"debut\",\n",
      " \"in\",\n",
      " \"2001,\",\n",
      " \"with\",\n",
      " \"the\",\n",
      " \"very\",\n",
      " \"talented\",\n",
      " \"wordsmith\",\n",
      " \"Alan\",\n",
      " \"Cox.\",\n",
      " \"It\",\n",
      " \"was\",\n",
      " \"a\",\n",
      " \"silly\",\n",
      " \"story\",\n",
      " \"about\",\n",
      " \"potholes.\",\n",
      " \"It's\",\n",
      " \"been\",\n",
      " \"more\",\n",
      " \"than\",\n",
      " \"two\",\n",
      " \"decades\",\n",
      " \"and\",\n",
      " \"they\",\n",
      " \"haven't\",\n",
      " \"invited\",\n",
      " \"me\",\n",
      " \"back\",\n",
      " \"on...but\",\n",
      " \"they\",\n",
      " \"haven't\",\n",
      " \"kicked\",\n",
      " \"me\",\n",
      " \"out\",\n",
      " \"either.\",\n",
      " \"\\ud83e\\udd23\",\n",
      " \"#FaceForRadio\",\n",
      " \"#ThrowbackThursday\",\n",
      " \"https://t.co/GDLJK7xvdl\",\n",
      " \"RT\",\n",
      " \"@Redacted_DMV:\",\n",
      " \"#ThrowBackThursday\",\n",
      " \"to\",\n",
      " \"the\",\n",
      " \"link\",\n",
      " \"up\",\n",
      " \"with\",\n",
      " \"@bottom_boy_dmv\",\n",
      " \"Need\",\n",
      " \"another\",\n",
      " \"round\",\n",
      " \"asap\",\n",
      " \"\\ud83d\\ude0f#DMVFreaks\",\n",
      " \"#DCfreaks\",\n",
      " \"https://t.co/Y1wZ1SM\\u2026\",\n",
      " \"RT\",\n",
      " \"@RicFlairNatrBoy:\",\n",
      " \"The\",\n",
      " \"Two\",\n",
      " \"Nature\",\n",
      " \"Boys!\",\n",
      " \"WOOOOO!\",\n",
      " \"#ThrowbackThursday\",\n",
      " \"https://t.co/butqv4N6oV\",\n",
      " \"RT\",\n",
      " \"@reba:\",\n",
      " \"You\",\n",
      " \"know\",\n",
      " \"I\",\n",
      " \"love\",\n",
      " \"a\",\n",
      " \"#throwbackthursday!!!\",\n",
      " \"Sandi\",\n",
      " \"Spika\",\n",
      " \"designed\",\n",
      " \"this\",\n",
      " \"fringed\",\n",
      " \"green\",\n",
      " \"western\",\n",
      " \"suit\",\n",
      " \"for\",\n",
      " \"my\",\n",
      " \"1995\",\n",
      " \"tour\",\n",
      " \"and\",\n",
      " \"we\",\n",
      " \"pulled\",\n",
      " \"it\",\n",
      " \"ou\\u2026\",\n",
      " \"RT\",\n",
      " \"@COfficialOgbeni:\",\n",
      " \"#ThrowbackThursday\",\n",
      " \"In\",\n",
      " \"2012\",\n",
      " \"Asiwaju\",\n",
      " \"Bola\",\n",
      " \"Tinubu\",\n",
      " \"Received\",\n",
      " \"The\",\n",
      " \"Highest\",\n",
      " \"Award\",\n",
      " \"From\",\n",
      " \"The\",\n",
      " \"U.S.\",\n",
      " \"Black\",\n",
      " \"Mayors\",\n",
      " \"Presented\",\n",
      " \"By\",\n",
      " \"The\\u2026\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "status_texts = [ search_result['id'] for search_result in search_results.data ]\n",
    "# the escape character \"\\\" allows for the list comprehension to continue\n",
    "# on a new line. While not strictly necessary, it makes code more readable\n",
    "# for your fellow programmers.\n",
    "\n",
    "status_texts = [ search_result['text'] for search_result in search_results.data ]\n",
    "\n",
    "#hashtags = [ hashtag['text'] for status in statuses \\\n",
    "#        for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w for t in status_texts for w in t.split() ] #split the string on the empty spaces\n",
    "\n",
    "# Explore the first 5 items for each...\n",
    "print(json.dumps(status_texts, indent=1))\n",
    "print(json.dumps(words, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "You are now ready to parse usernames, hashtags and text from the results you previously obtained in Task 2 (e.g. `statuses_2`). While doing it, make sure to leave the variables created in 2.2 untouched. Instead, create your own variable names, which you'll be using soon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [226], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m tweets_text \u001b[38;5;241m=\u001b[39m [texts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m texts \u001b[38;5;129;01min\u001b[39;00m statuses2\u001b[38;5;241m.\u001b[39mdata]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#words = [ w for t in status_texts for w in t.split() \u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m tweets_hash_tag \u001b[38;5;241m=\u001b[39m [hash_tag \u001b[38;5;28;01mfor\u001b[39;00m textss \u001b[38;5;129;01min\u001b[39;00m tweets_text \u001b[38;5;28;01mfor\u001b[39;00m hash_tag \u001b[38;5;129;01min\u001b[39;00m textss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhashtags\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(tweets_text[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m], indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#print(json.dumps(tweets_hash_tag[0:3], indent=1))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [226], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m tweets_text \u001b[38;5;241m=\u001b[39m [texts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m texts \u001b[38;5;129;01min\u001b[39;00m statuses2\u001b[38;5;241m.\u001b[39mdata]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#words = [ w for t in status_texts for w in t.split() \u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m tweets_hash_tag \u001b[38;5;241m=\u001b[39m [hash_tag \u001b[38;5;28;01mfor\u001b[39;00m textss \u001b[38;5;129;01min\u001b[39;00m tweets_text \u001b[38;5;28;01mfor\u001b[39;00m hash_tag \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtextss\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentities\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhashtags\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(tweets_text[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m], indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#print(json.dumps(tweets_hash_tag[0:3], indent=1))\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "''''status_texts = [ search_result['id'] for search_result in statuses2.data ]\n",
    "for ids in status_texts:\n",
    "    user = twapi.get_user(user_id=ids)\n",
    "print(json.dumps(status_texts, indent=1))'''\n",
    "'''_id=1590473005372944386\n",
    "#_id=client.get_user(*,user_id)\n",
    "_id=1590473005372944386\n",
    "#_id=client.get_user(*,user_id)\n",
    "user=client.get_user(id=\"1590659974476500992\")\n",
    "print(user)'''\n",
    "tweets_text = [texts['text'] for texts in statuses2.data]\n",
    "\n",
    "#words = [ w for t in status_texts for w in t.split() \n",
    "tweets_hash_tag = [hash_tag for textss in tweets_text for hash_tag in textss['entities']['hashtags']]\n",
    "\n",
    "#rint(json.dumps(tweets_text[0:3], indent=1))\n",
    "\n",
    "#print(json.dumps(tweets_hash_tag[0:3], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating a basic frequency distribution from words in tweets\n",
    "*(from Examples 1-7 in Mining the Social Web)* \n",
    "\n",
    "\n",
    "In the cell below we display the 10 most common hashtag instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('RT', 7), ('the', 7), ('#ThrowbackThursday', 6), ('I', 6), ('to', 4), ('of', 4), ('my', 3), ('me', 3), ('a', 3), ('in', 3)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for item in [words]:\n",
    "    c = Counter(item)\n",
    "    \n",
    "print(c.most_common()[:10]) # top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look something like this: \\\n",
    "`[('ThrowbackThursday', 34), ('throwbackthursday', 11), ('TBT', 6), ('ThrowBackThursday', 6), ('Trivia', 3), ('madoka_magica', 2), ('New', 2), ('EURO2020', 2), ('artists', 2)]`\n",
    "\n",
    "### Task 4\n",
    "Show hashtags frequency for results that you obtained in Task 3. Think about possible explanations for the different results you get from the analyses for the different queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Storing your results\n",
    "So far, we have been storing the data in working memory. Often it's handy to store your data to disk so you can retrieve it in a next session. \n",
    "\n",
    "The pickle module lets you do exactly that, by serializing data in a binary format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filepath = \"my_data.pickle\"\n",
    "# this indented python syntax is broadly defined as \"context manager\".\n",
    "# This means that everything happening under its indentation will use f\n",
    "# as file handle to filepath. The Shortand `wb` stands for \"write binary\",\n",
    "# which is how we serialize data to disk.\n",
    "with open(filepath, \"wb\") as f:\n",
    "    pickle.dump(words, f) # write the contents of list 'words' to file 'f'\n",
    "    \n",
    "# Note that, after the end of the indented block, the file is automatically closed.\n",
    "# Hence, no memory resource on your system is wasted idly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you browse to your working directory, you should find a file there named \"myData.pickle\". You can open this in a text editor, or load its contents back into a variable to do some more analyses on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the myData.pickle file and store its contents into variable 'words'\n",
    "\n",
    "with open(filepath, \"rb\") as f:\n",
    "    words = pickle.load(f)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using prettytable to display tuples in a nice way\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "pt = PrettyTable(field_names=['Words', 'Count'])\n",
    "c = Counter(words)\n",
    "[ pt.add_row(kv) for kv in c.most_common()[:10] ]\n",
    "pt.align[\"Words\"], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "print(pt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Calculating lexical diversity for tweets \n",
    "*(from Example 1-9 in Mining the Social Web)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for computing lexical diversity\n",
    "def lexical_diversity(tokens):\n",
    "    return 1.0*len(set(tokens))/len(tokens)\n",
    "\n",
    "# Define a function for computing the average number of words per tweet\n",
    "def average_words(statuses):\n",
    "    total_words = sum([ len(s.split()) for s in status_texts ])\n",
    "    return 1.0*total_words/len(statuses) \n",
    "\n",
    "# Let's use these functions:\n",
    "\n",
    "print(lexical_diversity(words))\n",
    "print(average_words(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: What do the printed numbers indicate? Try to explain them.\n",
    "\n",
    "(*Double click this cell to write your answer*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Looking up users who have retweeted a status \n",
    "*(from Example 1-11 in Mining the Social Web):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets = client.get_retweeters(id=1224140327688249349) # Get the original tweet id for a tweet from its retweeted_status node and insert it here\n",
    "print(\"Users who've retweeted the tweet:\\n\")\n",
    "print([retweets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 (advanced)\n",
    "\n",
    "If you have a Twitter account with a nontrivial number of tweets, request your historical tweet archive from your account settings and analyze it. \\\n",
    "The export of your account data includes files organized by time period in a convenient JSON format. See the README.txt file included in the downloaded archive for more details. \n",
    "\n",
    "\n",
    "\n",
    "What are the most common terms that appear in your tweets? \\\n",
    "Who do you retweet the most often? \\\n",
    "How many of your tweets are retweeted (and why do you think this is the case)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Plotting frequencies of words \n",
    "*(from Example 1-12 in Mining the Social Web)*\n",
    "\n",
    "\n",
    "In the previous exercises we have been looking at the text from the tweets, but when you retrieved the results, you retrieved much more information about the tweets, such as the username of the person who shared this tweet with the world. \n",
    "\n",
    "\n",
    "You can use this information to find out who retweets whom in our examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = sorted(Counter(words).values(), reverse=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.loglog(word_counts)\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.xlabel(\"Word Rank\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating histograms of words, screen names, and hashtags \n",
    "*(from Example 1-13 in Mining the Social Web):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(words)\n",
    "plt.hist(c.values())\n",
    "    \n",
    "plt.title(\"\")\n",
    "plt.ylabel(\"Number of items in bin\")\n",
    "plt.xlabel(\"Bins (number of times an item appeared)\")\n",
    "    \n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra: seaborn plots with a one-liner.\n",
    "!pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(word_counts, kde=False, rug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
